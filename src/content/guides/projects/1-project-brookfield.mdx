---
title: "Project: EnergyIQ – Smart Energy Usage Dashboard"
description: "A real-world Java project showcasing microservices architecture, Spring Boot, and AWS implementation for Brookfield Renewable's energy monitoring system."
author: "Study Docs"
category: "guides"
subcategory: "projects"
---

# EnergyIQ – Detailed Project Explanation

## Introduction
I’m **Srinivas Alahari**, a Full Stack Developer with over 5 years of experience in designing and building scalable microservices-based applications. Currently at **Brookfield Renewable** in Toronto, I specialize in Java Spring Boot backends, React.js frontends, and cloud-native deployments on AWS. In this project, I led the migration of legacy energy monitoring tools to a modern platform, delivering real-time insights and driving operational efficiencies.


Based on the sources provided, here is a detailed explanation of the EnergyIQ project, connecting concepts mentioned in the references:

## Project Overview
Your project involved migrating Brookfield Renewable’s legacy energy monitoring tools to a modern, scalable platform to provide corporate customers with **real-time insights into their energy usage**. This initiative aimed to promote sustainability and optimize power consumption. The existing tools were outdated, non‑scalable, and suffered from performance bottlenecks.

## Technical Implementation
- **Microservices Architecture**
  The system is composed of independent Spring Boot microservices, each responsible for a distinct business function. Spring Cloud was used to manage service discovery, traffic balancing, and secret management.

- **Docker for Containerization**
  Services were packaged as Docker images (via Dockerfile specifying Java version and application JAR) and deployed on AWS EC2 using CloudFormation templates to automate infrastructure provisioning.

## Data Flow & Architecture Components

1. **IoT Devices (D1 – Smart Meter Device)**
   Energy meters generate voltage, current, and power usage readings as the initial data source.

2. **Edge Gateway (D2)**
   Smart meters **publish readings** to an Edge Gateway which aggregates and forwards data.

3. **Messaging Layer**
   - **Apache Kafka Topic (K)**: Edge Gateway pushes events to Kafka, acting as a high-throughput message broker for energy data.
   - **AWS SNS & SQS**: Kafka integrates with SNS (publish notifications) and SQS (durable queuing) for asynchronous message handling and decoupling producers/consumers.
   - **Energy Ingestion Service (E1)** subscribes to Kafka to receive raw meter readings.

4. **Backend Services**
   - **Energy Ingestion Service (E1)** (Spring Boot, Java 11): Subscribes to Kafka, **stores raw data** in **MongoDB (MG)**, and **publishes alerts** via AWS SNS.
   - **Analytics Engine (A1)**: Triggered by SQS messages (alerts), processes and **stores metrics** in **PostgreSQL (PG)**.
   - **User Service (U1)**: Manages user profiles and preferences, **reads user settings** from PostgreSQL, and exposes RESTful APIs for the dashboard.

   ### How E1 Sends Data to A1
After ingesting meter readings, **E1 publishes events** in two ways:
1. **Kafka Publish**: E1’s Kafka producer writes each reading event or batch event to a predefined topic. A1 runs a Kafka consumer that subscribes to this topic, receives new events in real time, and processes them.
2. **SNS→SQS Workflow**: E1 also pushes notifications to an AWS SNS topic; that SNS topic is subscribed by an SQS queue. A1 polls this SQS queue for new messages, ensuring durable, ordered delivery and enabling retries.

> **Note:** The Analytics Engine (A1) does **not** call the Energy Ingestion Service (E1) directly via REST or RPC. All communication is handled asynchronously by consuming events from Kafka topics or polling the SQS queue.

**Example: Voltage Reading Flow**
- **Sample Payload** (JSON):
  ```json
  {
    "meterId": "SM-12345",
    "timestamp": "2025-04-30T14:35:00Z",
    "readings": {
      "voltage": 230.5,
      "current": 5.2,
      "power": 1200
    }
  }
  ```
- **Step-by-Step**:
  1. The Edge Gateway publishes this JSON to the Kafka topic `energy-readings`.
  2. **Ingestion Service** (E1) consumes the message, validates the payload, and writes it to MongoDB:
     ```java
     @KafkaListener(topics = "energy-readings")
     public void onMessage(ReadingEvent event) {
         // save raw data
         mongoTemplate.insert(event, "raw_readings");
         // publish alert if voltage > threshold
         if (event.getReadings().getVoltage() > 240) {
             snsClient.publish(alertTopicArn, event.toString());
         }
     }
     ```
  3. E1’s Kafka producer then **re-publishes** the same or enriched event to `analytics-readings` topic.
  4. **Analytics Engine** (A1) consumes from `analytics-readings`, transforms the data (e.g., computes rolling average), and writes metrics to PostgreSQL:
     ```java
     @KafkaListener(topics = "analytics-readings")
     public void processReading(ReadingEvent event) {
         double rollingAvg = computeRollingAverage(event);
         jdbcTemplate.update("INSERT INTO metrics (meter_id, timestamp, voltage_avg) VALUES (?, ?, ?)",
             event.getMeterId(), event.getTimestamp(), rollingAvg);
     }
     ```

This example shows how a single voltage reading flows from ingestion through analytics, demonstrating both code-level handling and end-to-end data movement.

This dual-path design ensures high throughput (Kafka) plus reliable, decoupled processing (SNS/SQS). A1 picks up messages from either source, applies analytics logic, and writes results to PostgreSQL. (Kafka) plus reliable, decoupled processing (SNS/SQS). A1 picks up messages from either source, applies analytics logic, and writes results to PostgreSQL.

### Detailed SNS → SQS Workflow Steps
1. **Alert Publishing**: Within the `onMessage` listener, when a voltage reading exceeds the threshold, E1 calls:
   ```java
   snsClient.publish(alertTopicArn, eventJson);
   ```
2. **SNS Fan-out**: AWS SNS receives the `alertTopicArn` publish and immediately pushes the message to all its subscribers, one of which is an SQS queue subscription.
3. **SQS Ingestion**: The subscribed SQS queue stores the message durably. If SNS delivery fails, it retries (default retry policy).
4. **Polling by Analytics Engine**: A1 polls the SQS queue at regular intervals (e.g., every 500ms):
   ```java
   ReceiveMessageRequest request = new ReceiveMessageRequest(sqsQueueUrl)
       .withMaxNumberOfMessages(10)
       .withWaitTimeSeconds(20);
   List<Message> messages = sqsClient.receiveMessage(request).getMessages();
   ```
5. **Message Processing**: For each received `Message`, A1:
   - Deserializes the JSON payload.
   - Performs analytics computations.
   - Writes metrics to PostgreSQL.
6. **Message Deletion**: After successful processing, A1 deletes the message to prevent reprocessing:
   ```java
   sqsClient.deleteMessage(new DeleteMessageRequest(sqsQueueUrl, message.getReceiptHandle()));
   ```
7. **Error Handling & Retries**:
   - If A1 fails to process a message (e.g., DB timeout), the message becomes visible again after the visibility timeout (default 30s).
   - After a configurable number of retries (e.g., 3), messages can be moved to a dead-letter queue (DLQ) for manual inspection.

This step-by-step SNS → SQS flow ensures that critical alert messages are delivered reliably and processed exactly once by the Analytics Engine.

5. **Database Layer**
   - **MongoDB (MG)**: Time-series storage for raw meter readings.
   - **PostgreSQL (PG)**: Structured storage for aggregated metrics and user preferences.

6. **Frontend**
   - **React.js Dashboard (R)**: Visualizes real-time usage, carbon footprint, and cost trends.
   - Communicates via RESTful APIs to U1 and A1.
   - Utilizes reusable components and lazy loading to improve performance.
 **Frontend**
   - **React.js Dashboard (R)**: Visualizes real‑time usage, carbon footprint, and cost trends.
   - Communicates via RESTful APIs to U1 and A1.
   - Utilizes reusable components and lazy loading to improve performance.

## Relevant Spring Boot Concepts & Practices

- **Microservices & Communication**: REST templates, Feign Client, or message brokers (Kafka/RabbitMQ) for inter‑service calls.
- **Spring Boot Simplifications**: `@SpringBootApplication` (combines configuration, auto‑configuration, component scanning), JPA Repositories, and conditional configuration (`@ConditionalOnClass`).
- **REST API Best Practices**: Versioning via URLs, headers, or media types; secured with OAuth2 and Spring Security.
- **Security**: OAuth2 for token‑based authentication, externalized secrets via Spring Cloud Config or Vault, API Gateway for routing and rate limiting.
- **Performance Optimization**: Kafka’s async messaging for low latency, optimized DB queries, thread‑pool tuning, caching (`@Cacheable`), and async methods (`@Async`).
- **CI/CD & Deployment**: Jenkins + Docker + Git with Canary deployments, CloudFormation for infrastructure as code.
- **Monitoring & Troubleshooting**: Spring Boot Actuator, distributed tracing (Sleuth/Zipkin), ELK stack for centralized logging.
- **Externalized Configuration**: YAML, environment variables, Spring Cloud Config.
- **Testing**: Unit tests (JUnit/Mockito), integration tests (`@SpringBootTest`), mocks (WireMock).
- **Resilience**: Circuit breakers and retries (Resilience4j), timeouts, bulkheads.
- **Event‑Driven Architecture**: Custom events (`ApplicationEvent`), listeners (`@EventListener`), Kafka/SNS/SQS for cross‑service events.

## Timeline & Team
- **Duration**: 7 months
- **Team**: 6 members (2 frontend, 2 backend, 1 DevOps, 1 QA)
- **Methodology**: Agile Scrum with 2‑week sprints, daily standups, Jira tracking, code reviews, and sprint demos.

## Business Impact
- **20% reduction** in operational costs post-migration to AWS.
- **25% improvement** in dashboard response time.
- Enabled sustainability reporting, boosting enterprise client engagement.

---

## Q&A: SNS/SQS Fallback Behavior
**Q: Does SNS push alerts to A1 when Kafka fails, and then A1 gets data from SQS?**
> **Answer:**
> - **Parallel Publishing:** The Ingestion Service (E1) publishes each event to **both** Kafka and SNS at the same time, not only on Kafka failure.
> - **Primary Path:** A1 normally consumes events directly from the Kafka topic for real-time analytics.
> - **Fallback Path:** Simultaneously, E1 publishes to an SNS topic which fans out to an SQS queue. A1 continuously polls this SQS queue as a fallback.
> - **Failure Scenario:** If Kafka becomes unavailable, the SQS-backed path still buffers all incoming events. A1 processes messages from SQS at its regular polling interval, ensuring no data loss.
> - **Consumer Logic:** A1 can deduplicate if it processes the same event from both Kafka and SQS (e.g., by checking unique IDs).

This setup guarantees continuous data delivery and fault tolerance, using SQS as a reliable backup when Kafka is down.


---

**15. What React hooks were used in the EnergyIQ frontend?**
> **Answer:**
> - **useState**: For managing component-level state, such as selected time range or chart visibility.
> - **useEffect**: To fetch dashboard data on mount and re-fetch when filters change—handles side effects like API calls.
> - **useContext**: To access global contexts, e.g., theme settings or authentication tokens, without prop drilling.
> - **useMemo**: To memoize expensive calculations (like processing large arrays of time-series data) and avoid re-computing on every render.
> - **useCallback**: To memoize event handlers passed to child components (e.g., filter change handlers), preventing unnecessary re-renders.
> - **useRef**: To hold mutable values (like a reference to a DOM element for dynamic chart resizing) without triggering re-renders.
> - **useReducer**: For managing complex state logic in a structured way, such as multi-step filter forms or batch data manipulations.

> **Concept & Why It Works:**
> Hooks allow functional components to manage state and lifecycle events cleanly, replacing class-based patterns and improving code readability and reuse.

> **Use Case in EnergyIQ:**
> - `useState` kept track of user-selected date ranges and alert toggles.
> - `useEffect` triggered data fetches from the User Service whenever selections changed.
> - `useContext` provided app-wide theme (light/dark) and user authorization context for API calls.
> - `useMemo` optimized chart data transformations so charts render smoothly even with high-frequency updates.
> - `useCallback` stabilized callback references passed into a chart component’s props to avoid unnecessary redraws.
> - `useRef` captured chart container dimensions to implement a responsive resizing behavior.
> - `useReducer` managed state transitions for a complex filter panel that combined multiple criteria.

> **Possible Follow-Up Questions:**
> - How do you decide between `useState` and `useReducer`?
> - What pitfalls should you watch for when using `useEffect` dependencies?
> - How does `useMemo` help with performance, and when might it hurt it?

